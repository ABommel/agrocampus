{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Plots\n",
    "using LinearAlgebra\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared error loss function\n",
    "loss(w, b, x, y) = sum(abs2, y - (w*x .+ b)) / size(y, 2)\n",
    "\n",
    "# get gradient w.r.t to `w`\n",
    "loss∇w(w, b, x, y) = ForwardDiff.gradient(w -> loss(w, b, x, y), w)\n",
    "\n",
    "# get derivative w.r.t to `b` (`ForwardDiff.derivative` is\n",
    "# used instead of `ForwardDiff.gradient` because `b` is\n",
    "# a scalar instead of an array)\n",
    "lossdb(w, b, x, y) = ForwardDiff.derivative(b -> loss(w, b, x, y), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient descent function\n",
    "function train(w, b, x, y; lr=0.1)\n",
    "    w -= lmul!(lr, loss∇w(w, b, x, y))\n",
    "    b -= lr * lossdb(w, b, x, y)\n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 10\n",
    "x = randn(n, p)'\n",
    "y = sum(x[1:5,:]; dims=1) .+ randn(n)' * 0.1\n",
    "w = 0.0001 * randn(1, p)\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "err = Float64[]\n",
    "for i = 1:50\n",
    "   w, b = train(w, b, x, y)\n",
    "   push!(err, loss(w,b,x,y))\n",
    "end\n",
    "plot(err)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "julia",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
